{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\Admin\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "OpenCV Version: 4.10.0\n",
      "NumPy Version: 1.26.4\n",
      "Ultralytics YOLO Imported Successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"OpenCV Version:\", cv2.__version__)\n",
    "print(\"NumPy Version:\", np.__version__)\n",
    "print(\"Ultralytics YOLO Imported Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 117.8ms\n",
      "Speed: 3.4ms preprocess, 117.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 104.2ms\n",
      "Speed: 3.0ms preprocess, 104.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 106.2ms\n",
      "Speed: 3.4ms preprocess, 106.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 170.4ms\n",
      "Speed: 4.3ms preprocess, 170.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 103.0ms\n",
      "Speed: 1.4ms preprocess, 103.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 150.8ms\n",
      "Speed: 2.2ms preprocess, 150.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 128.2ms\n",
      "Speed: 1.9ms preprocess, 128.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 106.9ms\n",
      "Speed: 2.3ms preprocess, 106.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 118.1ms\n",
      "Speed: 3.6ms preprocess, 118.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 106.4ms\n",
      "Speed: 2.1ms preprocess, 106.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 124.2ms\n",
      "Speed: 1.4ms preprocess, 124.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 120.2ms\n",
      "Speed: 2.4ms preprocess, 120.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 108.3ms\n",
      "Speed: 1.8ms preprocess, 108.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 93.9ms\n",
      "Speed: 1.7ms preprocess, 93.9ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 104.1ms\n",
      "Speed: 2.1ms preprocess, 104.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 128.8ms\n",
      "Speed: 4.0ms preprocess, 128.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 127.9ms\n",
      "Speed: 3.4ms preprocess, 127.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 152.9ms\n",
      "Speed: 2.7ms preprocess, 152.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 109.0ms\n",
      "Speed: 1.3ms preprocess, 109.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 106.4ms\n",
      "Speed: 1.4ms preprocess, 106.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 92.4ms\n",
      "Speed: 1.5ms preprocess, 92.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 116.5ms\n",
      "Speed: 2.3ms preprocess, 116.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 113.7ms\n",
      "Speed: 2.1ms preprocess, 113.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 125.3ms\n",
      "Speed: 5.0ms preprocess, 125.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 90.9ms\n",
      "Speed: 2.0ms preprocess, 90.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 93.8ms\n",
      "Speed: 1.3ms preprocess, 93.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 90.0ms\n",
      "Speed: 2.0ms preprocess, 90.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 94.1ms\n",
      "Speed: 1.4ms preprocess, 94.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 114.8ms\n",
      "Speed: 3.4ms preprocess, 114.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 97.0ms\n",
      "Speed: 1.6ms preprocess, 97.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 103.8ms\n",
      "Speed: 2.8ms preprocess, 103.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 102.2ms\n",
      "Speed: 3.6ms preprocess, 102.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 95.6ms\n",
      "Speed: 2.8ms preprocess, 95.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 88.1ms\n",
      "Speed: 2.8ms preprocess, 88.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 81.7ms\n",
      "Speed: 1.3ms preprocess, 81.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 84.2ms\n",
      "Speed: 2.1ms preprocess, 84.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 2 persons, 84.9ms\n",
      "Speed: 3.1ms preprocess, 84.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 3 persons, 100.8ms\n",
      "Speed: 3.9ms preprocess, 100.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 3 persons, 110.0ms\n",
      "Speed: 3.2ms preprocess, 110.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 97.4ms\n",
      "Speed: 2.4ms preprocess, 97.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 104.2ms\n",
      "Speed: 3.2ms preprocess, 104.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 103.3ms\n",
      "Speed: 3.0ms preprocess, 103.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 107.9ms\n",
      "Speed: 4.3ms preprocess, 107.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 97.3ms\n",
      "Speed: 1.5ms preprocess, 97.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 97.2ms\n",
      "Speed: 3.4ms preprocess, 97.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 2 persons, 110.3ms\n",
      "Speed: 2.3ms preprocess, 110.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 (no detections), 102.1ms\n",
      "Speed: 2.2ms preprocess, 102.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 95.0ms\n",
      "Speed: 2.0ms preprocess, 95.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 2 tvs, 106.5ms\n",
      "Speed: 2.2ms preprocess, 106.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 2 tvs, 111.2ms\n",
      "Speed: 3.3ms preprocess, 111.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 121.2ms\n",
      "Speed: 3.3ms preprocess, 121.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 106.9ms\n",
      "Speed: 1.6ms preprocess, 106.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 102.6ms\n",
      "Speed: 2.5ms preprocess, 102.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 108.9ms\n",
      "Speed: 2.8ms preprocess, 108.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 89.3ms\n",
      "Speed: 1.3ms preprocess, 89.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 91.7ms\n",
      "Speed: 1.5ms preprocess, 91.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 106.4ms\n",
      "Speed: 3.1ms preprocess, 106.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 108.7ms\n",
      "Speed: 2.4ms preprocess, 108.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 laptop, 116.2ms\n",
      "Speed: 2.1ms preprocess, 116.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 cat, 1 tv, 1 laptop, 106.9ms\n",
      "Speed: 2.8ms preprocess, 106.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 tv, 98.4ms\n",
      "Speed: 1.5ms preprocess, 98.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 106.4ms\n",
      "Speed: 3.0ms preprocess, 106.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 98.7ms\n",
      "Speed: 2.9ms preprocess, 98.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 99.6ms\n",
      "Speed: 1.3ms preprocess, 99.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 1 tv, 85.3ms\n",
      "Speed: 1.6ms preprocess, 85.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 86.9ms\n",
      "Speed: 1.6ms preprocess, 86.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 tv, 99.1ms\n",
      "Speed: 1.5ms preprocess, 99.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 104.7ms\n",
      "Speed: 2.9ms preprocess, 104.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 3 cars, 1 tv, 121.2ms\n",
      "Speed: 2.5ms preprocess, 121.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3}\n",
      "\n",
      "0: 480x640 1 person, 3 cars, 1 tv, 116.1ms\n",
      "Speed: 1.9ms preprocess, 116.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 105.1ms\n",
      "Speed: 1.4ms preprocess, 105.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 tv, 99.2ms\n",
      "Speed: 1.6ms preprocess, 99.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 2 tvs, 107.2ms\n",
      "Speed: 2.8ms preprocess, 107.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 105.8ms\n",
      "Speed: 1.5ms preprocess, 105.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 109.7ms\n",
      "Speed: 1.7ms preprocess, 109.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 tv, 126.5ms\n",
      "Speed: 4.0ms preprocess, 126.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 car, 1 tv, 110.9ms\n",
      "Speed: 2.6ms preprocess, 110.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 100.3ms\n",
      "Speed: 1.3ms preprocess, 100.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 93.6ms\n",
      "Speed: 2.4ms preprocess, 93.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 1 tv, 121.5ms\n",
      "Speed: 2.3ms preprocess, 121.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 traffic light, 97.3ms\n",
      "Speed: 1.2ms preprocess, 97.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 tv, 100.6ms\n",
      "Speed: 1.5ms preprocess, 100.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 traffic light, 1 tv, 99.2ms\n",
      "Speed: 2.1ms preprocess, 99.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 109.0ms\n",
      "Speed: 1.9ms preprocess, 109.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 traffic light, 1 tv, 121.6ms\n",
      "Speed: 2.4ms preprocess, 121.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 traffic light, 1 tv, 103.3ms\n",
      "Speed: 1.5ms preprocess, 103.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 2 traffic lights, 1 tv, 100.6ms\n",
      "Speed: 1.4ms preprocess, 100.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 traffic light, 1 tv, 102.7ms\n",
      "Speed: 1.3ms preprocess, 102.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 3 persons, 2 cars, 1 tv, 114.4ms\n",
      "Speed: 1.6ms preprocess, 114.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 3 persons, 2 cars, 1 traffic light, 1 tv, 109.6ms\n",
      "Speed: 1.6ms preprocess, 109.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 traffic light, 1 tv, 106.1ms\n",
      "Speed: 1.5ms preprocess, 106.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 tv, 131.1ms\n",
      "Speed: 3.0ms preprocess, 131.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 3 cars, 1 traffic light, 1 tv, 106.0ms\n",
      "Speed: 2.1ms preprocess, 106.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 131.2ms\n",
      "Speed: 3.9ms preprocess, 131.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 102.8ms\n",
      "Speed: 2.0ms preprocess, 102.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 104.2ms\n",
      "Speed: 3.4ms preprocess, 104.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 92.6ms\n",
      "Speed: 2.4ms preprocess, 92.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 bus, 1 tv, 113.7ms\n",
      "Speed: 3.1ms preprocess, 113.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 bus, 1 tv, 143.1ms\n",
      "Speed: 2.2ms preprocess, 143.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 bus, 1 tv, 107.5ms\n",
      "Speed: 2.4ms preprocess, 107.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 122.9ms\n",
      "Speed: 2.4ms preprocess, 122.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 107.0ms\n",
      "Speed: 1.6ms preprocess, 107.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 1 cell phone, 113.2ms\n",
      "Speed: 2.1ms preprocess, 113.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 bus, 1 tv, 110.8ms\n",
      "Speed: 2.4ms preprocess, 110.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2, 'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 1 cell phone, 118.0ms\n",
      "Speed: 2.5ms preprocess, 118.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 3 cars, 1 tv, 107.9ms\n",
      "Speed: 1.7ms preprocess, 107.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 1 tv, 125.2ms\n",
      "Speed: 2.8ms preprocess, 125.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 103.5ms\n",
      "Speed: 4.5ms preprocess, 103.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 100.8ms\n",
      "Speed: 1.3ms preprocess, 100.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 truck, 1 tv, 112.9ms\n",
      "Speed: 1.9ms preprocess, 112.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 truck, 1 tv, 105.4ms\n",
      "Speed: 2.3ms preprocess, 105.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 truck, 1 tv, 103.4ms\n",
      "Speed: 1.6ms preprocess, 103.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 truck, 1 tv, 105.5ms\n",
      "Speed: 2.7ms preprocess, 105.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 truck, 1 tv, 109.7ms\n",
      "Speed: 1.3ms preprocess, 109.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 truck, 1 tv, 116.6ms\n",
      "Speed: 3.1ms preprocess, 116.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 105.1ms\n",
      "Speed: 1.5ms preprocess, 105.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 130.7ms\n",
      "Speed: 3.5ms preprocess, 130.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 2 cars, 1 tv, 100.8ms\n",
      "Speed: 2.5ms preprocess, 100.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 truck, 1 tv, 147.4ms\n",
      "Speed: 1.6ms preprocess, 147.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 2 tvs, 139.1ms\n",
      "Speed: 3.2ms preprocess, 139.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 car, 1 tv, 87.1ms\n",
      "Speed: 1.9ms preprocess, 87.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 tv, 83.9ms\n",
      "Speed: 1.3ms preprocess, 83.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 car, 1 tv, 106.4ms\n",
      "Speed: 2.6ms preprocess, 106.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 2 cars, 1 tv, 97.3ms\n",
      "Speed: 1.3ms preprocess, 97.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 car, 1 tv, 109.1ms\n",
      "Speed: 1.8ms preprocess, 109.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 3 cars, 1 truck, 2 tvs, 130.7ms\n",
      "Speed: 3.8ms preprocess, 130.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 2, 'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 4 cars, 1 traffic light, 1 tv, 105.3ms\n",
      "Speed: 1.7ms preprocess, 105.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3, 'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 4 cars, 1 traffic light, 1 tv, 93.3ms\n",
      "Speed: 1.4ms preprocess, 93.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3, 'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 3 cars, 1 truck, 1 tv, 107.9ms\n",
      "Speed: 2.4ms preprocess, 107.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 3, 'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 1 car, 1 truck, 1 tv, 98.8ms\n",
      "Speed: 2.7ms preprocess, 98.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'car': 1, 'bus': 1, 'truck': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 82.5ms\n",
      "Speed: 1.4ms preprocess, 82.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 82.2ms\n",
      "Speed: 1.4ms preprocess, 82.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 tv, 1 cell phone, 114.0ms\n",
      "Speed: 1.9ms preprocess, 114.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 cell phone, 88.0ms\n",
      "Speed: 1.4ms preprocess, 88.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 cell phone, 86.2ms\n",
      "Speed: 1.7ms preprocess, 86.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 1 cell phone, 99.5ms\n",
      "Speed: 1.4ms preprocess, 99.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 99.1ms\n",
      "Speed: 2.4ms preprocess, 99.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 tv, 99.8ms\n",
      "Speed: 2.6ms preprocess, 99.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 1 tv, 118.1ms\n",
      "Speed: 2.6ms preprocess, 118.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 100.0ms\n",
      "Speed: 1.8ms preprocess, 100.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 96.7ms\n",
      "Speed: 1.9ms preprocess, 96.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 3 persons, 1 cell phone, 120.2ms\n",
      "Speed: 2.4ms preprocess, 120.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 95.0ms\n",
      "Speed: 1.8ms preprocess, 95.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 2 persons, 1 cell phone, 131.3ms\n",
      "Speed: 5.0ms preprocess, 131.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 cat, 128.5ms\n",
      "Speed: 1.4ms preprocess, 128.5ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 125.4ms\n",
      "Speed: 1.7ms preprocess, 125.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 112.4ms\n",
      "Speed: 3.0ms preprocess, 112.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 95.5ms\n",
      "Speed: 1.7ms preprocess, 95.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 108.1ms\n",
      "Speed: 1.8ms preprocess, 108.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 92.3ms\n",
      "Speed: 1.3ms preprocess, 92.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 102.5ms\n",
      "Speed: 2.3ms preprocess, 102.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 131.7ms\n",
      "Speed: 1.7ms preprocess, 131.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 1 refrigerator, 107.5ms\n",
      "Speed: 1.4ms preprocess, 107.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 121.9ms\n",
      "Speed: 1.9ms preprocess, 121.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 87.3ms\n",
      "Speed: 2.3ms preprocess, 87.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 88.1ms\n",
      "Speed: 2.2ms preprocess, 88.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 103.0ms\n",
      "Speed: 1.6ms preprocess, 103.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 97.0ms\n",
      "Speed: 2.4ms preprocess, 97.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 97.7ms\n",
      "Speed: 2.0ms preprocess, 97.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 85.2ms\n",
      "Speed: 2.0ms preprocess, 85.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 92.0ms\n",
      "Speed: 1.3ms preprocess, 92.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 99.1ms\n",
      "Speed: 2.2ms preprocess, 99.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 92.6ms\n",
      "Speed: 1.4ms preprocess, 92.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 107.3ms\n",
      "Speed: 2.5ms preprocess, 107.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 120.5ms\n",
      "Speed: 4.0ms preprocess, 120.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 113.9ms\n",
      "Speed: 1.9ms preprocess, 113.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 148.0ms\n",
      "Speed: 2.1ms preprocess, 148.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 1.9ms preprocess, 135.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 96.2ms\n",
      "Speed: 2.3ms preprocess, 96.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 100.9ms\n",
      "Speed: 1.3ms preprocess, 100.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 127.1ms\n",
      "Speed: 3.2ms preprocess, 127.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 146.5ms\n",
      "Speed: 2.4ms preprocess, 146.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 138.0ms\n",
      "Speed: 2.7ms preprocess, 138.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 1 cat, 138.1ms\n",
      "Speed: 2.6ms preprocess, 138.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 134.1ms\n",
      "Speed: 3.0ms preprocess, 134.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 112.4ms\n",
      "Speed: 2.0ms preprocess, 112.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 101.6ms\n",
      "Speed: 2.7ms preprocess, 101.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 108.4ms\n",
      "Speed: 3.4ms preprocess, 108.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 167.9ms\n",
      "Speed: 1.7ms preprocess, 167.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 130.5ms\n",
      "Speed: 2.5ms preprocess, 130.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 119.5ms\n",
      "Speed: 1.6ms preprocess, 119.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 110.7ms\n",
      "Speed: 1.3ms preprocess, 110.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 119.4ms\n",
      "Speed: 2.1ms preprocess, 119.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 102.9ms\n",
      "Speed: 1.8ms preprocess, 102.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 135.4ms\n",
      "Speed: 2.1ms preprocess, 135.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 104.3ms\n",
      "Speed: 1.8ms preprocess, 104.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 112.6ms\n",
      "Speed: 1.3ms preprocess, 112.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n",
      "0: 480x640 1 person, 148.4ms\n",
      "Speed: 2.7ms preprocess, 148.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "{'bus': 1}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Run YOLO object detection\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Temporary storage for new vehicle positions\u001b[39;00m\n\u001b[0;32m     32\u001b[0m new_tracked_vehicles \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\engine\\model.py:182\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\engine\\model.py:550\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 550\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:214\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:323\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 323\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:171\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    166\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    167\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    170\u001b[0m )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:568\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 568\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:114\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:132\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:153\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    154\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:91\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Initialize video capture (0 for webcam, or replace with video file path)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Store detected vehicles (ID -> (x, y, type))\n",
    "tracked_vehicles = {}\n",
    "vehicle_id_counter = 0\n",
    "\n",
    "# Active vehicle count per type\n",
    "active_vehicles = defaultdict(int)\n",
    "\n",
    "def get_vehicle_counts():\n",
    "    \"\"\"Returns the current count of detected vehicles dynamically.\"\"\"\n",
    "    return {k: v for k, v in active_vehicles.items() if v > 0}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run YOLO object detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Temporary storage for new vehicle positions\n",
    "    new_tracked_vehicles = {}\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            # Only consider vehicle types\n",
    "            if class_name in {\"car\", \"truck\", \"bus\", \"motorcycle\"}:\n",
    "\n",
    "                # Check if this vehicle is already tracked\n",
    "                assigned_id = next(\n",
    "                    (vid for vid, (px, py, v_type) in tracked_vehicles.items()\n",
    "                     if abs(center_x - px) < 50 and abs(center_y - py) < 50), \n",
    "                    None\n",
    "                )\n",
    "\n",
    "                # Assign new ID if it's a new vehicle\n",
    "                if assigned_id is None:\n",
    "                    assigned_id = vehicle_id_counter\n",
    "                    vehicle_id_counter += 1\n",
    "                    active_vehicles[class_name] += 1\n",
    "\n",
    "                # Store new position\n",
    "                new_tracked_vehicles[assigned_id] = (center_x, center_y, class_name)\n",
    "\n",
    "                # Draw bounding box & label\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"{class_name} {assigned_id}\", (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                cv2.circle(frame, (center_x, center_y), 5, (255, 255, 255), -1)\n",
    "\n",
    "    # Update vehicle counts for exiting vehicles\n",
    "    for vid, (_, _, v_type) in tracked_vehicles.items():\n",
    "        if vid not in new_tracked_vehicles:\n",
    "            active_vehicles[v_type] = max(0, active_vehicles[v_type] - 1)\n",
    "\n",
    "    # Update tracked vehicles\n",
    "    tracked_vehicles = new_tracked_vehicles\n",
    "\n",
    "    # Display vehicle counts on screen\n",
    "    for i, (v_type, count) in enumerate(get_vehicle_counts().items()):\n",
    "        cv2.putText(frame, f\"{v_type}: {count}\", (30, 50 + i * 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "    # Show frame with detections\n",
    "    cv2.imshow(\"Vehicle Detection\", frame)\n",
    "\n",
    "    # Print the latest vehicle count dynamically\n",
    "    print(get_vehicle_counts())\n",
    "\n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Final count after video ends\n",
    "print(\"Final Count:\", get_vehicle_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
